---
title: "Logistic Regression"
tutorial:
  id: "18-Logit"
output: 
  learnr::tutorial:
    progressive: true
    allow_skip: true
runtime: shiny_prerendered
description: >
  Learn how to estimate and make predictions from models where the outcome is a binary variable.
---

## Learning Objectives

In this tutorial, you will learn how to conduct logistic regression analysis in R. Specifically we will cover:

* How to use the `glm()` function to estimate a logistic regression
* How to use the `summary()` function to present the results of logistic regression analysis
* How to use the regression output to test hypotheses about the intercept and slope using the five-step hypothesis testing procedure
* How to use the `predict()` function to generate predictions from a logistic regression model
* How to assess model fit using the information from the `hitmiss()` function in the package <span style="color:green">**pscl**</span> 



```{r setup, include=FALSE}
library(learnr)
library(tidyverse)
library(pscl)
#library(knitr)
library(poliscidata)
library(gradethis)
tutorial_options(exercise.checker = gradethis::grade_learnr)
#tutorial_options(exercise.timelimit = 60)
knitr::opts_chunk$set(error = TRUE)
counties <- qpaTutorials::counties
counties$dem2p_percent <- counties$dem2p_vote_share*100
counties$wage_growth <- counties$wage_growth*100
counties$black_percent <- counties$prop_black*100
counties$White[counties$prop_white>0.5] <- 1
counties$White[counties$prop_white<=0.5] <- 0
df <- qpaTutorials::df
df$region <- factor(df$region, labels =  c("South", "Northeast", "Midwest", "West"))
library(poliscidata)
library(stargazer)
states <- qpaTutorials::states
model1 <- glm(guncontrol_opencarry ~ republican, data = states, family = "binomial")
aNES <- qpaTutorials::aNES

#First assign a "Democrat" to all cases where Dem is equal to 1
aNES$party[aNES$Dem==1] <- "Democrat"
#Next assign a "Repbulican" to all cases where GOP is equal to 1
aNES$party[aNES$GOP==1] <- "Republican"
#Then assign "Neither" to all cases where both Dem and GOP are equal to 0
aNES$party[(aNES$GOP==0 & aNES$Dem==0)] <- "Neither"
#Change the order of the factor 
aNES$partyf <- factor(aNES$party, levels = c("Neither", "Democrat", "Republican"))
aNES$educ[aNES$educ==9] <=NA
aNES$femalef <- factor(aNES$female, labels=c("Male","Female"))
model2 <- glm(Trump2016 ~  Blacks.FT + Illegals.FT + partyf + femalef + educ, family = "binomial", data = aNES)

aNES <- aNES %>%
  mutate(turnout = ifelse(Trump2016 == 1, 1,
                        ifelse(Clinton2016 == 1, 1, 0)))
aNES <- aNES %>%
  mutate(strong = ifelse(pid_strength == 1, 1,
                        ifelse(pid_strength == 7, 1, 0)))
model3 <- glm(turnout~strong + income + educ + femalef, family = "binomial", data=aNES)
```

## Overview

If we are interested in predicting a binary categorical variable -- whether or not someone voted in an election, whether or not a state allows recreational marijuana use, whether or not a country experienced a terrorist attack in some time period, and the like -- we should not use linear regression. Why?  First, in most cases we have reason to believe the relationship between our explanatory variables and the outcome is non-linear such that a line doesn't fit the data well. In particular, a one-unit change in $X$ is likely to have a bigger impact for some values of $X$ than others. Second, the error terms will not be normally distributed because the outcome variable only takes two values, 0 and 1. This means hypothesis tests are not valid. Finally, the predicted probabilities generated by fitting a linear regression to a binary outcome variable can be greater than 1 or less than 0, which makes no sense.

<span style="color:blue">**Logistic regression**</span> is the standard way to model a binary outcome. Like with linear regression, our explanatory variables can be measured at the categorical or interval (or ratio) level.  Logistic regression models the probability that the outcome, $Y$, is equal to one given the explanatory variables, denoted $Pr(Y_i=1|X_{ki})$, using the logistic function:

$$Pr(Y_i=1) = \frac{exp(\alpha+ \beta X_i)}{1+ exp(\alpha + \beta X_i)}$$.

where $exp$ represents the transcendental number 2.718... taken to some power. So, $exp(\alpha+ \beta X_i)$ is the same as $e^{\alpha+ \beta X_i}$. $\alpha$ is still called the intercept and $\beta$ the slope.

The logitistic function is nonlinear. This means that we are no longer fitting a line to the data. Instead we are fitting a curve. This results in some complications for interpreting the substantive effects of our explanatory variables and necessitates a different approach to assessing model fit.


The `glm()` function in base R will estimate a logistic regression model. We will illustrate how to use the `glm()` function to estimate a model, how to use the `summary()` function to produce a summary of the output, how to use the `predict()` function to generate predictions from the model, how to use the `hitmiss()` function in the <span style="color:green">pscl</span> package to assess model fit, and we will practice interpreting results using several examples. In the tutorial "Presenting Logistic Regression Results" we will cover additional tools for aiding in the interpretation of results from logistic regression.


## Example 1: Simple Logistic Regression

To illustrate logistic regression we will begin with a simple logistic regression with a single independent variable.

Let's say that we have posed the following question: How is the percent of a state's adults who are Republican related to the probability a state allows openly carrying a firearm in public? We might make the argument that Republicans tend to be more opposed to gun control than independents or Democrats, which should, in turn, make states with more Republicans more likely to allow open carry. Thus we hypothesize:

 <span style="color:Chocolate">In a comparison of states, those with more Republicans will be more likely to allow open carry than those states with fewer Republicans.</span>


The data frame **states** contains data for the year 2010 for each of the 50 states. Our outcome variable is **guncontrol_opencarry**, which is coded a 0 if a state did not allow open carry and a 1 if it did. Our explanatory variable is **republican**, which is the percent of a state's adults who claim to be Republicans.



### Estimate a Logistic Regression 

 Running a logistic regression in R is really quite simple! The function to run a regression is `glm()`, which stands for generalized linear model. We must specify three arguments to estimate the model. Just like with the `lm()` function we used to estimate linear regression, we must specify a `formula`, which lists the dependent variable followed by a tilde (~) and the independent variable. When we have more than one independent variable we add them with a plus sign. We must also specify the `data`, as we did when using linear regression. In addition, we must specify the `family` argument and set it equal to "binomial". This tells R to estimate a logistic regression.


Notice we don't need `states$` before the variable names, because we are using the `data` argument to tell R where the data is coming from. We are going to save the results as the object **model1** and then use the `summary()` function to present the output.


```{r first, exercise=TRUE}
model1 <- glm(guncontrol_opencarry ~ republican, data = states, family = "binomial")
summary(model1)
```

### Interpret the Estimated Coefficients
 
While linear regression estimates a linear relationship between $Y$ and $X$, logistic regression estimates a curvilinear relationship. This means that we **cannot** interpret the estimated slopes as the effect of a one-unit change in the independent variable on the probability $Y=1$.  (In fact, the slope gives the change in the log-odds that the dependent variable takes the value 1 for every unit increase in X.)

 We **can** say whether the relationship is negative or positive. Any negatively signed slope indicates a negative relationship and any positively signed slope denotes a positive relationship. Thus, the relationship in our example is positive: the higher the percentage of Republicans in a state, the greater the probability it allows open carry.


###  Conduct Hypothesis Tests
  
We conduct hypothesis testing for the intercept and slope identically to the linear regression case. Let's test whether the slope for **republican** is statistically significant.

**Step 1. State the null hypothesis.** and **Step 2. State the alternative hypothesis**.

```{r letter-a, echo=FALSE}
question("Which of the following statements are true?",
  answer("$H:_0\\beta\\ne0$ is the null hypothesis", correct=FALSE, message="The null hypothesis specifies the slope is equal to zero in the population "),
  answer("$H:_A\\beta\\ne0$ is the alternative hypothesis", correct = TRUE),
  answer("$H:_0\\beta=0$ is the null hypothesis", correct=TRUE),
  answer("$H:_A\\beta=0$ is the alternative hypothesis", correct = FALSE, message="The null hypothesis states that $\\beta=0$ in the population and the alternative hypothesis states that $\\beta\\ne0$ in the population.  "),
  allow_retry = TRUE,
  try_again = "Hint: There is more than one correct answer"
  )
```

**Step 3. Calculate the test statistic** and **Step 4. Translate the test statistic to a p-value**.

The test statistic and the associated p-value for our hypothesis are given in the logistic regression output. Rather than the t-value, logistic regression reports the z-value.  If the absolute value of  z is greater than 1.96, it is highly unlikely that the observed result differs from zero by chance and we can reject the null hypothesis with 95% confidence. If it is less than 1.96, the evidence is consistent with the null hypothesis and we fail to reject it. 



The results from above are included here for your reference.

```{r, echo=FALSE}
summary(model1)
```
**Step 5. Draw a conclusion.**

```{r letter-a2, echo=FALSE}
question("Examine the output above. Which of the following statements are true?",
    answer("We cannot reject the null hypothesis", correct=FALSE, message="The $p<0.05$ so we can reject the null hypothesis with 95% confidence. "),
  answer("The evidence tells us that there is a statistically significant positive relationship between the percentage of Republicans in a state and the likelihood it allows open carry.", correct=TRUE, message="Because the z-value is greater than 1.96 and $p<0.05$, we can reject the null hypothesis that $\\beta=0$ with 95% confidence. Therefore, we conclude there is a statistically significant positive relationship between the percentage of Republicans in a state and the likelihood it allows open carry."),
  allow_retry = TRUE
  )
```
 


### Generate Predictions from a Logit Model

We rely on predicted probabilities to interpret the results from a logistic regression. The predicted probability can be calculated from the formula for the logit model:

$$Pr(\hat{Y_i}=1|X_i) = \frac{\exp(\hat{\alpha} + \hat{\beta} X_i)}{1+ \exp(\hat{\alpha} + \hat{\beta} X_i)}$$
where $PR(\hat{Y_i}=1|X_i)$ is the probability $Y=1$, given the value of $X_i$.


Let's calculate the probability $Y=1$, that a state allows open carry, given a a state has 30% Republicans, $X_i=30$.

$$Pr(\hat{Y_i}=1|X_i=30) = \frac{\exp(-17.0391+ 0.6464*30)}{1+ \exp(-17.0391 + 0.6464*30)}$$

Below is code to have R do the algebra for us.

```{r pred1, exercise=TRUE}
(exp(-17.0391 + 0.6464 * 30)) / (1 + exp(-17.0391 + 0.6464 * 30))
```
In a state with 30% Republicans, the predicted probability the state has an open carry law is just over 91%.


To have R do this calculation directly for us, we can use the `predict()` function. The `predict()` function takes 3 arguments. The first is the name of the model fit object, here **model1**. We then need to pass the function the `newdata` argument. For this argument, we provide a data frame containing the name of the independent variable set equal to the value for which we want to calculate a prediction. The argument requires that the data be in a data frame, so we wrap this in the `data.frame()` function. The final argument is `type`, which we set equal to "response" to tell the function to return a predicted probability rather than the log-odds.

Look carefully at the code to ensure you understand what you need to do to generate a predicted value.


```{r predfun, exercise=TRUE}
predict(model1, newdata = data.frame(republican = 30), type = "response")
```

The table below presents the predicted probability for several values of $X$.

$X$ | Predicted
--- | --------- 
18 | 0.00 
19 | 0.01
20 | 0.02 
21 | 0.03
22 | 0.06 
23 | 0.10 
24 | 0.18
25 | 0.29 
26 | 0.44 
27 | 0.60 
28 | 0.74 
29 | 0.85
30 | 0.91 
31 | 0.95
32 | 0.97
35 | 1.00

Notice that the predicted probabilities increase in a nonlinear fashion. Small increases in the percent of Republicans have little effect at lower values and higher values: For all values of percent Republican below 18, the predicted probability a state has open carry is zero and for all values 35 and over it is 1.0. But a one percentage point increase has a large effect on the predicted probability a state allows open carry with values of **republican** in the 20s: The predicted probability a state allows open if there are 25% Republicans is 0.29 but with 30% Republicans it is 0.91. That's a (very large) 0.47 difference!

Below is a plot that illustrates the predicted values from the model. We will learn to create plots like these in the tutorial "Presenting Logistic Regression Results."

```{r, echo=FALSE, message=FALSE}
library(sjPlot)
library(sjmisc)
set_theme(
  geom.outline.color = "antiquewhite4", 
  geom.outline.size = 1, 
  geom.label.size = 2,
  geom.label.color = "grey50",
  title.color = "red", 
  title.size = 1.5, 
  axis.angle.x = 45, 
  axis.textcolor = "blue", 
  base = theme_bw()
)
NotFancy <- function(l) {
 l <- format(l, scientific = FALSE)
 parse(text=l)
}
plot_model(model1, 
           type="pred",
           terms = "republican [10:50]",
           show.data = TRUE,
           title="Predicted Probabilities of Open Carry Laws as a \nFunction of the Percent of Republicans a the State", 
           axis.title = c("Percent Republican","Predicted Probability")) +
  scale_y_continuous(labels=NotFancy) 
```


Notice that the predicted function is a curve rather than a line. That's because we estimated a logistic regression. For lower and higher values of the percent Republican, the curve is flat, but for values of percent Republican between 20 and 30 the curve is very steep, indicating that small changes in the percent Republican have a large impact on the predicted probability a state allows open carry.

The plot includes actual values of the $X,Y$ data pairs (a scatter plot is imposed on the plot) so we can also tell that there were some states for which our predictions were wrong. Notice that there is a state with almost 30% Republicans that did not allow open carry and there were several states with lower percentages of Republicans that did.

### Assess Model Fit


We assess the fit of a logit model differently than for a linear regression model. $R^2$ and the residual standard error we used to assess the fit of linear regression models have no direct equivalent in logit. Instead it is common to report the information from what's called a <span style="color:blue">confusion matrix</span>. We are particularly interested in two quantities: the <span style="color:blue">percent correctly predicted by the model</span> and the  <span style="color:blue">percent correctly predicted by the null model</span>. The difference between these two percentages tells us how much the model improves predictions over simply guessing the most common outcome category. You should always report both quantities and the net improvement in fit as a result of the explanatory variables in your model.

A <span style="color:blue">confusion matrix</span>  is a table that tells us how many times we observed y=0 (a state did not have an open carry law) and how many times we observed a y=1 (a state has an open carry law) in the data broken down by how many of those cases were and were not correctly predicted. 

The package  <span style="color:green">pscl</span> contains the function `hitmiss()` that produces a confusion matrix and measures of fit. It takes as its only argument the name of the model object that was fit. We must load the package with the `library()` function before using the function. Run the code below.


```{r hitmiss1, message=FALSE, exercise=TRUE}
library(pscl)
hitmiss(model1)   
```


 

What does this table tell us? 

1. The first piece of information is the classification threshold. It is 0.5, the default. This tells us that if the predicted probability was 0.5 or greater, the case was assigned to predict a value of one for the dependent variable, otherwise the prediction for that case was assigned a zero. 
2. Next is a cross tab where the rows of the table denote the values of $Y$ predicted by the model and the columns denote the actual values of $Y$ in the data. This is the confusion matrix.
    a. The logit model made a total of 50 predictions (one for each state). Out of those 50 cases, our model predicted 41 ones (the total in row 2 of the table) and 9 zeroes (total in row 1). 
    b. In fact, 40 states had open carry laws (40 ones, the total in column 2), and 10 did not (10 zeroes, the total in column 1).
3. The next three lines present different measures of model fit.
    a. The <span style="color:blue">**percent correctly predicted**</span> is exactly what it says. Our model correctly predicted 94% of the cases in the data set (47 out of 50). These are the cases on the diagonal from upper left to lower right. This seems really great! 
    b. The percent of zeroes that were correctly predicted. Here we correctly predict 80% of the states that did not allow open carry permits.
    c. The percent of ones that were correctly predicted. Here we correctly predicted 97.5% of the states that did allow open carry permits.
4. The last piece of information is the <span style="color:blue">**percent correctly predicted by the "Null Model."**</span> This is a model with no independent variables and is the percent of correct predictions when we just predict the modal category for every state.
    a.  80\% of the states allowed open carry permits in 2010. That means if we just guessed a state would have open carry permits -- without knowing the percent of Republicans in the state -- our percent correctly predicted would have been 80\%. 
    
The difference between the <span style="color:blue">**percent correctly predicted**</span> by the model and the  <span style="color:blue">**percent correctly predicted by the "Null Model"**</span> tells us our net improvement in predictions that is due to the explanatory variables. In our example, the net improvement is 94-80, or 14 percentage points. Not bad, but not great.


## Example 2: Logistic Regression with Multiple Independent Variables

We will seldom estimate a logistic regression with a single explanatory variable. It is important to control for competing explanations in any regression model if we hope to draw valid inferences. Extending logistic regression to the case with multiple independent variables is straightforward.

For this example we will use data from the 2016 American National Election Study in the data frame **aNES**. We are going to specify a model of vote for Donald Trump (**Trump2016**, coded 1 if an individual voted for Donald Trump and 0 otherwise) as a function of how warmly they feel toward Blacks on a feeling thermometer score coded from 0 to 100 where 0 is very cool and 100 is very warm (**Black.FT**) and illegal immigrants (**Illegals.FT**), which party they identify with (**partyf**, coded "Democrat", "Republican" and "Neither"), whether they are female (**femalef**, coded "Female" and "Male") and their level of education (**educ**, where 1 denotes a respondent did not complete high school, 2 denotes they have a high school diploma, 3 denotes they attended some college, trade, or business school, 4 denotes they have a college degree, and 5 denotes post graduate work). Note that we will treat **educ** as an interval level variable for the purposes of this illustration.

You likely have some hypotheses about who was most likely to vote for Trump, including hypotheses related to variables not included in this example. Consider the following:

<span style="color:Chocolate">In a comparison of individuals, we expect those who feel cooler toward Blacks wil have a higher likelihood of voting for Donald Trump than those who feel warmer toward Blacks.</span> (Because the Democratic Party tends to be more supportive of programs to benefit minorities.)

<span style="color:Chocolate">In a comparison of individuals, we expect those who feel cooler toward illegal immigrants will have a higher likelihood of voting for Donald Trump than those who feel warmer toward illegal immigrants.</span> (Because the Democratic Party has tended to be more supportive of immigrant rights.)

<span style="color:Chocolate">In a comparison of individuals, we expect Republicans will have a higher likelihood of voting for Donald Trump than either Democrats or those who identify with neither party.</span> (Because Donald Trump is a Republican.)

<span style="color:Chocolate">In a comparison of individuals, we expect males will have a higher likelihood of voting for Donald Trump than females.</span> (Because the Democratic candidate was a female and Trump made disparaging remarks about women during the campaign.)

Last but not least:

<span style="color:Chocolate">In a comparison of individuals, we expect those with lower levels of education will have a higher likelihood of voting for Donald Trump than those with higher levels of education.</span> (Because Donald Trump's promised to bring back jobs that are disproportionately held by those with lower levels of education.)

Let's see what we find.

### Estimate a Logistic Regression

Replace the XXX in the code below to estimate the logistic regression model. Both **femalef** and **partyf** are factor-class variables, so you do not need to enclose them inside the `factor()` function for this example.  


```{r logit2, exercise=TRUE}
model2 <- XXX(XXX ~  Blacks.FT + Illegals.FT + XXX + femalef + educ,  XXX = "XXX", XXX = XXX)
summary(model2)
```


```{r logit2-hint}
To estimate a logistic regression use the glm() function.
The dependent variable, Trump2016, is the first entry in the formula.
Make sure to add partyf to the formula and specify family="binomial".
Finally, specify the data argument as aNES.
```

```{r logit2-solution}
model2 <- glm(Trump2016 ~  Blacks.FT + Illegals.FT + partyf + femalef + educ, family = "binomial", data = aNES)
summary(model2)
```
```{r logit2-check}
grade_code()
```

### Interpret the Coefficients and Conduct Hypothesis Tests

Rather than walk separately through the 5 steps of hypothesis testing for each of our variables, examine the output and draw your conclusions about the direction and significance of the relationships in the model.

```{r letter-b, echo=FALSE}
question("Which of the following statements are true?",
  answer("We can be 95% confident that feelings toward illegal immigrants are significantly related to the probability of voting for Donald Trump", correct=TRUE, message="We can be 95% confident that feelings toward illegal immigrants are significantly related to the probability of voting for Donald Trump because $p<.05$. "),
  answer("Republicans are significantly more likely and Democrats significantly less likely than those who do not identify with either party to vote for Donald Trump", correct=TRUE, message="Because $p<.05$ for Republican and Democrat, we can reject the null hypothesis that the slopes are zero. Because these coefficients give the difference between each category and the baseline/omitted category, we can conclude Republicans are significantly more likely and Democrats significantly less likely than those who do not identify with either party to vote for Donald Trump. "),
  answer("For every one degree warmer an individual feels toward illegal immigrants, they were about 2.5% more likely to vote for Donald Trump, all else equal",  correct=FALSE, message="We cannot interpret the slope in a logistic regression as the effect of a unit change in $X$ on the probability $Y=1$. "),
  allow_retry = TRUE,
  try_again = "Hint: There is more than one correct answer"
  )
```

### Generate Predictions from the Model

We can generate predictions from our model using the `predict()` function. Below is the code to predict the probability an individual who sits at the middle of the feeling thermometer scale for Blacks and illegal immigrants, who is a female Democrat, and has a college degree. Run the code and then change the values of the independent variables and rerun the code to examine the predicted probabilities for other types of voters. Try to find a voter profile that results in a prediction of near 50%.


```{r predfun2, exercise=TRUE}
predict(model2, newdata = data.frame(Blacks.FT = 50, Illegals.FT = 50, partyf = "Democrat", femalef = "Female", educ=4), type = "response")
```


### Assess Model Fit

Recall that we assess model fit in a logistic regression based on the confusion matrix and that we can use the `hitmiss()` function in the <span style="color:green">pscl</span> package to produce the confusion matrix and associated measures of model fit. Replace the XXX in the code below to generate this information.

```{r hit2, exercise=TRUE}
XXX(XXX)
```

```{r hit2-hint}
Did you call the hitmiss() and pass it model2?
```

```{r hit2-solution}
hitmiss(model2)
```
```{r hit2-check}
grade_code()
```

```{r letter-bhit, echo=FALSE}
question("Look at the output above. Which of the following statements are true?",
  answer("The model predicted 2030 individuals did not vote for Donald Trump and 344 did", correct=FALSE, message="The total number of individuals predicted to vote for Donald Trump is given by summing the values in row 2 and the total number predicted to have not voted for Donald Trump is given by summing the values in row 1. "),
  answer("In the data, 2374 voters did not vote for Donald Trump and 1152 voters cast ballots for Donald Trump", correct=TRUE, message="The number of voters who did not vote for Donald Trump is the sum of the values in column 1 and the number who voted for him is the sum of the values in column 2. "),
  answer("The number of correct predictions based on the model is 2030+735 ",  correct=TRUE, message="Correct predictions are given on the diagonal, where yhat=y. "),
  answer("78.42-67.33=11.09 represents the improvement in our predictions based on the model over simply guessing that a respondent vote for Hillary Clinton", correct=TRUE, message="Because more voters voted for Hillary Clinton, that is the modal category and our model gets 11.09% more predictions correct than does guessing an individual voted for her."),
  answer("We correctly predicted 85.51% of the votes for Donald Trump", correct=FALSE, message="The model correctly predicted 85.51% of the zeroes, and 0 denoted a vote for Hillary Clinton."),
  allow_retry = TRUE,
  try_again = "Hint: There is more than one correct answer"
  )
```

## Example 3

For our final example, we will estimate a model of turnout (**turnout**, coded as 0 is an individual reported not voting and 1 if they reported voting) as a function of whether an individual is a strong partisan (**strong**, coded as 1 is strong, 0 is not strong), income (**income**, an ordinal variable coded from 1-6 based on 6 income categories), level of education completed (**educ**), and sex (**femalef**). We will treat both **educ** and **income** as interval variables in this example. This simplifying assumption implies we are willing to argue that the differences between categories are the same. Ideally, we'd include age in our model, but it is not in this small version of the American National Election Study data set. We will begin with the following hypotheses:

<span style="color:Chocolate">In a comparison of individuals, those who are strong partisans will be more likely to vote than those who are non-partisan or weak partisans.</span>  (Because strong partisans are more likely to be interested in politics and more concerned about the outcome.)

<span style="color:Chocolate">In a comparison of individuals, those who have higher incomes will be more likely to vote than those who have lower incomes.</span>  (Because income bestows resources that may allow individuals to participate more easily.)

<span style="color:Chocolate">In a comparison of individuals, those who have higher levels of education will be more likely to vote than those who have lower levels of education.</span>  (Because those with higher levels of education may be more knowledgeable about politics.)

It is unclear whether males or females will be more likely to vote. Which sex has turned out at higher rates has varied over time.

### Estimate the Logistic Regression

Complete the code to estimate the logistic regression. Then use the `summary()` function to present your results.

```{r model3, exercise=TRUE}
model3 <- 
```
```{r model3-hint}
Use the glm() function and specify turnout as the dependent variable.
Include the tilde and then list strong, income, educ, and femalef,
separated by a plus sign.
Set family="binomial".
Specity the data argument as aNES.
```

```{r model3-solution}
model3 <- glm(turnout~strong + income + educ + femalef, family = "binomial", data = aNES)
summary(model3)
```

```{r model3-check}
grade_code()
```


### Interpret the Coefficients and Test Hypotheses

```{r letter-c, echo=FALSE}
question("Look at the output above. Which of the following statements is true?",
  answer("All variables in the model are positively and significantly related to whether or not someone turned out to vote in 2016", correct=FALSE, message="Sex does not have a significant impact on whether or not an individual turned out to vote as z<1.96 and $p>.05$. "),
  answer("For every additional level of education completed, an individual is about 25% more likely to vote", correct=FALSE, message="We cannot interpret the slope in a logistic regression as the effect of a unit change in $X$ on the probability. "),
  answer("Stong partisans are more likely to vote than are those individuals who are not strong partisans ",  correct=TRUE),
  allow_retry = TRUE
  )
```

### Generate Predictions

Generate code to predict the probability that an individual who is not a strong partisan (**strong** = 0), has a high income (**income** = 5), who has a college degree (**educ** = 4), and is a female (**femalef** = "Female") turns out to vote.

```{r predfun3, exercise=TRUE}

```

```{r predfun3-hint}
You need to use the predict() function.
Pass the function the name of the model fit object, model3.
Specify the newdata argument by setting it equal to data.frame()
Inside data.frame list each independent variable and set it to the
requested value. Separate each by a comma.
Finally, add a comma and specify type="response".
```

```{r predfun3-solution}
predict(model3, newdata = data.frame(strong = 1, income = 5, educ = 4, femalef = "Female"), type = "response")
```

```{r predfun3-check}
grade_code()
```


### Assess Model Fit



Finally, write code to produce a confusion matrix and measures of model fit.

```{r fit3, exercise=TRUE}

```

```{r fit3-hint}
Call the hitmiss() function and pass it the name of the model fit object.
```

```{r fit3-solution}
hitmiss(model3)
```
```{r fit3-check}
grade_code()
```


Assess the fit of the model.

```{r letter-d, echo=FALSE}
question("Which of the following statements are true?",
  answer("The model correctly predicted 717 of the individuals who did not turn out to vote and 1844 of the indviduals who did", correct=TRUE, message="Correct predictions are on the diagonal from upper left to lower right. "),
  answer("The model correctly predicted voting in 78.6% of the cases where an individual turned out to vote", correct=TRUE, message="The percent of ones (those who turned out) is 78.6% while the percent of those correctly predicted who did not turn out is 42.4%. "),
  answer("Guessing the most commonly reported value of the dependent variable (an individual did vote) we would be correct 58.11% of the time ",  correct=TRUE, message="The correct predictions under the null model refers to the percent we would guess correctly just guessing someone said they turned out to vote. "),
  answer("We correctly predicted whether or not someone turned out to vote based on the model 63.44% of the time", correct=TRUE, message="The percent correctly predicted tells us the percent of all predictions that were correct."),
  allow_retry = TRUE,
  try_again = "Hint: There is more than one correct answer"
  )
```

This is not a particularly good model. We might like to control for other demographic variables, but if we want better predictions of individual turnout we would also like to know something about individual levels of interest in politics.

## The Takeaways

Logistic regression estimates a nonlinear relationship between a set of predictors and a binary (two-category) outcome variable. 

Logistic regression is easy to conduct in R using the `glm()` function. The results of the analysis can then be examined with the `summary()` function.  The results provide the information needed to interpret the direction and statistical significance of the effect of the independent variables (but not the size of the effect), to test hypotheses about the intercept and slopes, and to calculate the predicted values. In order to assess model fit we need to examine the confusion matrix and the percent of cases of the outcome variable that are correctly predicted.

* We can only directly infer the sign and statistical significance of estimates from the output in a logistic regression. This is because the effects are not linear and are expressed in log-odds. We need to transform the coefficients to draw substantive interpretations about the size of effects. We can use the `predict()` function R to do this for us.
* Another consequence of the nonlinear nature of the effects of our explanatory variables is that the effects of an independent variable in logistic regression depend on the values of all other variables. This means that when we calculate predictions, the effects are not "all else equal." We have to specify the values each variable takes.
* Hypothesis tests on the intercept and slope in logistic regression are conducted in the same fashion as with linear regression. We specify the null and alternative hypotheses, calculate a test statistic -- here the z-value, translate the test statistic into a p-value, and draw our conclusions. If the absolute value of z is greater than 1.96, we can reject the null hypothesis and infer that the coefficient is significantly different from zero with 95% confidence. In the case of the slopes, we can conclude that the variable has a statistically significant impact on the likelihood of the outcome variable.
* $R^2$ and the residual standard error have no direct parallels in logistic regression. Instead we examine the confusion matrix, which presents a cross tab of the predicted versus the actual values of the outcome variable. From this table, we can calculate a number of interesting measures of fit. Generally, we are interested in the percent correctly predicted by the model in comparison to the null model -- a model with no explanatory variables, in which we guess the most common value of the outcome variable. In addition, it can be informative to know if we are better at predicting one outcome than the other. 

In the next tutorial, "Presenting Logistic Regression Results" we will illustrate how to use plots to generate predicted values for interesting combinations of the explanatory variables.

## Test Your Knowledge

```{r letter-e, echo=FALSE}
question("Select the statement that is true",
  answer("We can assess the direction and size of the substantive effect of an explanatory variable directly from the results of a logistic regression", correct=FALSE, message="We can assess the direction of the effect from the estimates from a logistic regression, but we need to generate predictions to interpret their substantive effect. "),
  answer("We can assess the direction and statistical significance of an explanatory variable directly from the results of a logistic regression", correct=TRUE),
  answer("We can assess model fit of a logistic regression using the $R^2$ ",  correct=FALSE, message="Model fit in a logistic regression is assessed based on the information in a confusion matrix. We generally focus on the percent correctly predicted by the model. "),
  allow_retry = TRUE)
```

```{r letter-f, echo=FALSE}
question("Select the statement that is true",
  answer("The effect of an explanatory variable in a logistic regression with multiple independent variables depends only on the values taken by that variable", correct=FALSE, message="Because the relationship is nonlinear, the effects of any explanatory variable depend both on the values it takes and the values taken by all other variables in the model. "),
  answer("The logistic regression equation implies that the effects of an explanatory variable will be the same, regardless of the values it takes ",  correct=FALSE, message="Because the relationship is nonlinear, the effects of any explanatory variable depend both on the values it takes and the values taken by all other variables in the model. "),
   answer("The effects of an explanatory variable in a logistic regression depend both on the values it takes and the values taken by all other variables in the model.", correct=TRUE),
  allow_retry = TRUE
  )
```

```{r letter-f2, echo=FALSE}
question("Select the statements that are true",
  answer("The percent correctly predicted by the null model is the percent correctly predicted by guessing the modal outcome category", correct=TRUE),
  answer("The difference between the percent correctly predicted by the model and the percent correctly predicted by the null model tells you how much including the explanatory variables in the model improves fit over guessing the modal outcome cateogry ",  correct=TRUE),
   answer("The percent correctly predicted by the null model is the percent of ones correctly predicted by the model", correct=FALSE, message="The percent correctly predicted by the null model is the percent correctly predicted by guessing the modal outcome category. "),
   answer("To assess how well the model fits the data we need only know the percent correctly predicted", correct=FALSE, message="To know how well the model fits the data it is important to know how well we would do just be guessing the modal outcome category. "),
  allow_retry = TRUE,
  try_again = "Hint: There is more than one correct answer"
  )
```




```{r letter-g, echo=FALSE}
question("Which of the following snippets of code will produce a predicted value for a logistic regression stored in model4 with two explanatory variables, $X$ and $Z$?",
  answer("predict(model4, newdata=data.frame(X=20, Z=10))", correct=FALSE, message="You need to specify the type of prediction. "),
  answer("predict(model4, newdata=data.frame(X=20, Z=10), type=\"response\")", correct=TRUE),
  answer("predict(model4, newdata=c(X=20, Z=10), type=\"response\") ",  correct=FALSE, message="You need to specify that the newdata is a data.frame. "),
  allow_retry = TRUE
  )
```


```{r letter-h, echo=FALSE}
question("Which of the following snippets of code will produce the information necessary to describe the fit of model4",
  answer("hitmiss(model4)", correct=TRUE),
  answer("summary(model4)", correct=FALSE, message="You need to generate a confusion matrix. "),
  answer("predict(model4, newdata=data.frame(X=20, Z=10), type=\"response\")",  correct=FALSE, message="This code generates predicted values but does not compare them to actual values. "),
  allow_retry = TRUE
  )
```